{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bade97c0-03c3-4230-97cc-dfa1252c66aa",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f0459a-a295-4859-85a2-b049bab7315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress specific Keras warnings from FER library\n",
    "warnings.filterwarnings(\"ignore\", message=\"The structure of `inputs` doesn't match the expected structure.\")\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from fer import FER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd5b7a-bb97-431d-9523-6389e8ffe7c5",
   "metadata": {},
   "source": [
    "Helping function for drawing of lines and shadows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015aea2f-c5a0-4364-bef8-f7059770bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for drawing clear text with shadow\n",
    "def draw_text_with_shadow(image, text, pos, font_scale, color, thickness):\n",
    "    x, y = pos\n",
    "    cv2.putText(image, text, (x+1, y+1), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale, (255,255,255), thickness+1, cv2.LINE_AA)\n",
    "    cv2.putText(image, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale, color, thickness, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35536bc-9f6b-478b-bd0b-e4189023b5ac",
   "metadata": {},
   "source": [
    "Function for checking the hand opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09a2f47-6ea8-42ed-982b-21d32dcdd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detect if hand is open by counting fingers raised and finger spread angle\n",
    "def is_hand_open(hand_landmarks, image_width, image_height):\n",
    "    wrist = hand_landmarks.landmark[0]\n",
    "    tips = [hand_landmarks.landmark[i] for i in [4,8,12,16,20]]\n",
    "    wrist_y = wrist.y\n",
    "    count_up = sum(lm.y < wrist_y for lm in tips[1:])\n",
    "    idx_tip = tips[1]\n",
    "    pinky_tip = tips[4]\n",
    "    idx_x, idx_y = int(idx_tip.x * image_width), int(idx_tip.y * image_height)\n",
    "    pinky_x, pinky_y = int(pinky_tip.x * image_width), int(pinky_tip.y * image_height)\n",
    "    wrist_x, wrist_y_px = int(wrist.x * image_width), int(wrist.y * image_height)\n",
    "    vec_idx = np.array([idx_x - wrist_x, idx_y - wrist_y_px])\n",
    "    vec_pinky = np.array([pinky_x - wrist_x, pinky_y - wrist_y_px])\n",
    "    vec_idx_norm = vec_idx / (np.linalg.norm(vec_idx) + 1e-6)\n",
    "    vec_pinky_norm = vec_pinky / (np.linalg.norm(vec_pinky) + 1e-6)\n",
    "    angle_rad = np.arccos(np.clip(np.dot(vec_idx_norm, vec_pinky_norm), -1.0, 1.0))\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "    return count_up >= 4 and angle_deg > 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cea29-bb56-4909-a2da-5284fc05caaf",
   "metadata": {},
   "source": [
    "Initializing Mediapipe utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6eee4c8-9215-4829-a19d-8b66bcf2ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe holistic and drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c282b-5ff2-4426-bc6a-dfd16eb984e6",
   "metadata": {},
   "source": [
    "Detection of facial landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "243f19cc-c78e-4913-95dd-07c351d879c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facial landmarks relevant for emotion heuristics\n",
    "important_face_landmarks = {\n",
    "    \"Forehead\": 10,\n",
    "    \"Eyes\": 33,\n",
    "    \"Nose\": 1,\n",
    "    \"Lips\": 61,\n",
    "    \"Chin\": 199,\n",
    "    \"Left Ear\": 127,\n",
    "    \"Right Ear\": 356,\n",
    "    # Add inner brows and eye lid points important for AUs\n",
    "    \"Left_Eyebrow_Inner\": 70,\n",
    "    \"Right_Eyebrow_Inner\": 300,\n",
    "    \"Left_Eye_Upper\": 159,\n",
    "    \"Left_Eye_Lower\": 145,\n",
    "    \"Right_Eye_Upper\": 386,\n",
    "    \"Right_Eye_Lower\": 374\n",
    "}\n",
    "important_body_landmarks = {\n",
    "    \"Left Shoulder\": 11,\n",
    "    \"Right Shoulder\": 12,\n",
    "    \"Left Elbow\": 13,\n",
    "    \"Right Elbow\": 14,\n",
    "    \"Left Wrist\": 15,\n",
    "    \"Right Wrist\": 16,\n",
    "    \"Left Hip\": 23,\n",
    "    \"Right Hip\": 24,\n",
    "    \"Left Knee\": 25,\n",
    "    \"Right Knee\": 26,\n",
    "    \"Left Ankle\": 27,\n",
    "    \"Right Ankle\": 28\n",
    "}\n",
    "\n",
    "# Finger landmarks for fingertip trails and labels\n",
    "finger_landmarks = {\n",
    "    0: \"Palm\",\n",
    "    4: \"Thumb Tip\",\n",
    "    8: \"Index Tip\",\n",
    "    12: \"Middle Tip\",\n",
    "    16: \"Ring Tip\",\n",
    "    20: \"Pinky Tip\"\n",
    "}\n",
    "\n",
    "offsets = {\n",
    "    \"Left\": (-25, -12),\n",
    "    \"Right\": (23, -10),\n",
    "    \"default\": (10, -14)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b513b-f2a9-466b-a5e7-bee640c649ec",
   "metadata": {},
   "source": [
    "Styling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "165ad2ff-bf03-487e-8032-6b00a383cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing parameters\n",
    "font_scale = 0.40\n",
    "line_thickness = 1\n",
    "circle_radius = 2\n",
    "text_thickness = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba807e0-1d93-4830-b12b-97d0da8ee501",
   "metadata": {},
   "source": [
    "Emotion Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dbf73f1-5ae1-4dc3-ac35-7f8cf8835223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FER detector with MTCNN for better accuracy\n",
    "emotion_detector = FER(mtcnn=True)\n",
    "\n",
    "# For fingertip trails visualization\n",
    "max_trails_len = 20\n",
    "finger_trails_right = {i: deque(maxlen=max_trails_len) for i in finger_landmarks.keys()}\n",
    "finger_trails_left = {i: deque(maxlen=max_trails_len) for i in finger_landmarks.keys()}\n",
    "\n",
    "# Utility: Smooth emotion label to reduce jitter\n",
    "emotion_history = deque(maxlen=10)\n",
    "def smooth_emotion(emotion_label):\n",
    "    emotion_history.append(emotion_label)\n",
    "    return max(set(emotion_history), key=emotion_history.count)\n",
    "\n",
    "# Emotion heuristics matching your detailed descriptions:\n",
    "\n",
    "def detect_happy(face_landmarks):\n",
    "    try:\n",
    "        left_lip_corner = face_landmarks.landmark[61]\n",
    "        right_lip_corner = face_landmarks.landmark[291]\n",
    "        nose_tip = face_landmarks.landmark[1]\n",
    "        left_cheek = face_landmarks.landmark[234]\n",
    "        right_cheek = face_landmarks.landmark[454]\n",
    "        left_eye_top = face_landmarks.landmark[159]\n",
    "        left_eye_bottom = face_landmarks.landmark[145]\n",
    "        lip_raise = (left_lip_corner.y < nose_tip.y) and (right_lip_corner.y < nose_tip.y)\n",
    "        eye_gap_left = abs(left_eye_top.y - left_eye_bottom.y)\n",
    "        eye_squint = eye_gap_left < 0.018\n",
    "        cheek_raise = (left_cheek.y < nose_tip.y) or (right_cheek.y < nose_tip.y)\n",
    "        return lip_raise and (eye_squint or cheek_raise)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def detect_sad(face_landmarks):\n",
    "    try:\n",
    "        left_brow_inner = face_landmarks.landmark[70]\n",
    "        right_brow_inner = face_landmarks.landmark[300]\n",
    "        left_eyebrow_center = face_landmarks.landmark[105]\n",
    "        right_eyebrow_center = face_landmarks.landmark[334]\n",
    "        mouth_left = face_landmarks.landmark[61]\n",
    "        mouth_right = face_landmarks.landmark[291]\n",
    "        eye_center = face_landmarks.landmark[168]\n",
    "        brows_raised = (left_brow_inner.y < left_eyebrow_center.y) and (right_brow_inner.y < right_eyebrow_center.y)\n",
    "        mouth_down = (mouth_left.y > eye_center.y) and (mouth_right.y > eye_center.y)\n",
    "        return brows_raised and mouth_down\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def detect_angry(face_landmarks):\n",
    "    try:\n",
    "        brow_left = face_landmarks.landmark[70]\n",
    "        brow_right = face_landmarks.landmark[300]\n",
    "        nose_tip = face_landmarks.landmark[1]\n",
    "        lip_top = face_landmarks.landmark[13]\n",
    "        lip_bottom = face_landmarks.landmark[14]\n",
    "        brow_lowered = (brow_left.y > face_landmarks.landmark[159].y) and (brow_right.y > face_landmarks.landmark[386].y)\n",
    "        nose_wrinkle = (nose_tip.y < face_landmarks.landmark[10].y)\n",
    "        lips_pressed = abs(lip_top.y - lip_bottom.y) < 0.01\n",
    "        return brow_lowered and nose_wrinkle and lips_pressed\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def detect_nervous(face_landmarks):\n",
    "    try:\n",
    "        lip_bottom = face_landmarks.landmark[14]\n",
    "        lip_top = face_landmarks.landmark[13]\n",
    "        lips_pressed = abs(lip_top.y - lip_bottom.y) < 0.015\n",
    "        inner_brow_left = face_landmarks.landmark[70]\n",
    "        inner_brow_right = face_landmarks.landmark[300]\n",
    "        brows_contracted = abs(inner_brow_left.x - inner_brow_right.x) < 0.05\n",
    "        return lips_pressed and brows_contracted\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def detect_sleepy(face_landmarks):\n",
    "    try:\n",
    "        left_eye_top = face_landmarks.landmark[159]\n",
    "        left_eye_bottom = face_landmarks.landmark[145]\n",
    "        right_eye_top = face_landmarks.landmark[386]\n",
    "        right_eye_bottom = face_landmarks.landmark[374]\n",
    "        mouth_top = face_landmarks.landmark[13]\n",
    "        mouth_bottom = face_landmarks.landmark[14]\n",
    "        eye_open_left = abs(left_eye_top.y - left_eye_bottom.y)\n",
    "        eye_open_right = abs(right_eye_top.y - right_eye_bottom.y)\n",
    "        mouth_open = abs(mouth_top.y - mouth_bottom.y) > 0.02\n",
    "        return eye_open_left < 0.008 and eye_open_right < 0.008 and not mouth_open\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def detect_active(face_landmarks):\n",
    "    try:\n",
    "        left_eye_top = face_landmarks.landmark[159]\n",
    "        left_eye_bottom = face_landmarks.landmark[145]\n",
    "        right_eye_top = face_landmarks.landmark[386]\n",
    "        right_eye_bottom = face_landmarks.landmark[374]\n",
    "        left_brow = face_landmarks.landmark[70]\n",
    "        right_brow = face_landmarks.landmark[300]\n",
    "        eye_open_left = abs(left_eye_top.y - left_eye_bottom.y)\n",
    "        eye_open_right = abs(right_eye_top.y - right_eye_bottom.y)\n",
    "        brow_raised = (left_brow.y < face_landmarks.landmark[10].y) and (right_brow.y < face_landmarks.landmark[10].y)\n",
    "        return eye_open_left > 0.02 and eye_open_right > 0.02 and brow_raised\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def detect_anxious(face_landmarks):\n",
    "    try:\n",
    "        left_eye_top = face_landmarks.landmark[159]\n",
    "        left_eye_bottom = face_landmarks.landmark[145]\n",
    "        right_eye_top = face_landmarks.landmark[386]\n",
    "        right_eye_bottom = face_landmarks.landmark[374]\n",
    "        left_brow = face_landmarks.landmark[70]\n",
    "        right_brow = face_landmarks.landmark[300]\n",
    "        lip_top = face_landmarks.landmark[13]\n",
    "        lip_bottom = face_landmarks.landmark[14]\n",
    "        eye_open_left = abs(left_eye_top.y - left_eye_bottom.y)\n",
    "        eye_open_right = abs(right_eye_top.y - right_eye_bottom.y)\n",
    "        brow_furrow = abs(left_brow.x - right_brow.x) < 0.08\n",
    "        lips_pursed = abs(lip_top.y - lip_bottom.y) < 0.01\n",
    "        return eye_open_left > 0.025 and eye_open_right > 0.025 and brow_furrow and lips_pursed\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Custom heuristic to detect sleepy or active states based on eye openness landmarks\n",
    "def detect_sleepy_active(face_landmarks, width, height):\n",
    "    try:\n",
    "        left_eye_upper = face_landmarks.landmark[159]\n",
    "        left_eye_lower = face_landmarks.landmark[145]\n",
    "        right_eye_upper = face_landmarks.landmark[386]\n",
    "        right_eye_lower = face_landmarks.landmark[374]\n",
    "\n",
    "        left_eye_open = abs(left_eye_upper.y - left_eye_lower.y)\n",
    "        right_eye_open = abs(right_eye_upper.y - right_eye_lower.y)\n",
    "        avg_eye_open = (left_eye_open + right_eye_open) / 2\n",
    "\n",
    "        sleepy_threshold = 0.008\n",
    "        active_threshold = 0.02\n",
    "\n",
    "        if avg_eye_open < sleepy_threshold:\n",
    "            return \"Sleepy\"\n",
    "        elif avg_eye_open > active_threshold:\n",
    "            return \"Active\"\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def classify_detailed_emotion(face_landmarks, fer_emotion):\n",
    "    # Priority to heuristics based on your detailed emotion definitions\n",
    "    if face_landmarks:\n",
    "        if detect_happy(face_landmarks): return \"Happy\"\n",
    "        if detect_sad(face_landmarks): return \"Sad\"\n",
    "        if detect_angry(face_landmarks): return \"Angry\"\n",
    "        if detect_nervous(face_landmarks): return \"Nervous\"\n",
    "        if detect_sleepy(face_landmarks): return \"Sleepy\"\n",
    "        if detect_active(face_landmarks): return \"Active\"\n",
    "        if detect_anxious(face_landmarks): return \"Anxious\"\n",
    "    return fer_emotion  # fallback to FER prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d74d7b-2c90-4704-a02e-5559f2bf5902",
   "metadata": {},
   "source": [
    "Setting up the webcam (upto full screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867bd6e1-271e-4ce1-a7ce-6ac3bd36ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize webcam and MediaPipe holistic model\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "window_name = 'Advanced FER with Detailed Annotations'\n",
    "cv2.namedWindow(window_name, cv2.WND_PROP_FULLSCREEN)\n",
    "cv2.setWindowProperty(window_name, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "paused = False\n",
    "img_count = 0\n",
    "theme_idx = 0\n",
    "\n",
    "color_themes = [\n",
    "    {'dot': (0,0,255), 'line': (0,0,0)},\n",
    "    {'dot': (0,255,0), 'line': (160,32,240)},\n",
    "    {'dot': (255,127,36), 'line': (30,30,30)}\n",
    "]\n",
    "\n",
    "emotion_frame_skip = 5\n",
    "frame_counter = 0\n",
    "cached_emotions = []\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        if not paused:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_counter += 1\n",
    "\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image_rgb)\n",
    "            image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "            h, w, _ = image.shape\n",
    "            color_theme = color_themes[theme_idx]\n",
    "\n",
    "            # Draw important facial landmarks with labels\n",
    "            if results.face_landmarks:\n",
    "                for name, idx in important_face_landmarks.items():\n",
    "                    try:\n",
    "                        lm = results.face_landmarks.landmark[idx]\n",
    "                        x, y = int(lm.x * w), int(lm.y * h)\n",
    "                        cv2.circle(image, (x, y), circle_radius, color_theme['dot'], -1, cv2.LINE_AA)\n",
    "                        dx, dy = offsets[\"default\"]\n",
    "                        draw_text_with_shadow(image, name, (x+dx, y+dy), font_scale, (0,0,0), text_thickness)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            # Draw and label body landmarks and pose connections\n",
    "            if results.pose_landmarks:\n",
    "                for name, idx in important_body_landmarks.items():\n",
    "                    try:\n",
    "                        lm = results.pose_landmarks.landmark[idx]\n",
    "                        x, y = int(lm.x * w), int(lm.y * h)\n",
    "                        cv2.circle(image, (x, y), circle_radius+1, color_theme['dot'], -1, cv2.LINE_AA)\n",
    "                        if \"Left\" in name:\n",
    "                            dx, dy = offsets[\"Left\"]\n",
    "                        elif \"Right\" in name:\n",
    "                            dx, dy = offsets[\"Right\"]\n",
    "                        else:\n",
    "                            dx, dy = offsets[\"default\"]\n",
    "                        draw_text_with_shadow(image, name, (x+dx, y+dy), font_scale, (0,0,0), text_thickness)\n",
    "                    except:\n",
    "                        pass\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_holistic.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness, circle_radius=circle_radius),\n",
    "                    mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness)\n",
    "                )\n",
    "\n",
    "            # Process hands for fingertip trails and detect open hands\n",
    "            hands_open_messages = []\n",
    "            for hand_side, hand_landmarks, finger_trails in [\n",
    "                (\"Right\", results.right_hand_landmarks, finger_trails_right),\n",
    "                (\"Left\", results.left_hand_landmarks, finger_trails_left)\n",
    "            ]:\n",
    "                if hand_landmarks:\n",
    "                    for idx, part in finger_landmarks.items():\n",
    "                        try:\n",
    "                            lm = hand_landmarks.landmark[idx]\n",
    "                            x,y = int(lm.x * w), int(lm.y * h)\n",
    "                            finger_trails[idx].append((x,y))\n",
    "                            cv2.circle(image, (x,y), circle_radius+1, color_theme['dot'], -1, cv2.LINE_AA)\n",
    "                            dx, dy = offsets[hand_side]\n",
    "                            draw_text_with_shadow(image, f\"{hand_side} {part}\", (x+dx, y+dy), font_scale, (0,0,0), text_thickness)\n",
    "                        except:\n",
    "                            pass\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp_holistic.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness, circle_radius=circle_radius),\n",
    "                        mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness)\n",
    "                    )\n",
    "                    # Detect open hand but do NOT draw wrist-fingertip yellow lines\n",
    "                    if is_hand_open(hand_landmarks, w, h):\n",
    "                        hands_open_messages.append(f\"{hand_side} Hand Open\")\n",
    "                    # Draw fingertip trails as lines\n",
    "                    for trail in finger_trails.values():\n",
    "                        for i in range(1, len(trail)):\n",
    "                            cv2.line(image, trail[i-1], trail[i], (0,255,255), 1)\n",
    "\n",
    "            # Display open hand messages on screen\n",
    "            base_y = 140\n",
    "            for i, msg in enumerate(hands_open_messages):\n",
    "                draw_text_with_shadow(image, msg, (15, base_y + i*30), 0.7, (0,255,0), 2)\n",
    "\n",
    "            # Estimate and display wrist distance if both hands detected\n",
    "            if results.left_hand_landmarks and results.right_hand_landmarks:\n",
    "                lw = np.array([results.left_hand_landmarks.landmark[0].x,\n",
    "                               results.left_hand_landmarks.landmark[0].y,\n",
    "                               results.left_hand_landmarks.landmark[0].z])\n",
    "                rw = np.array([results.right_hand_landmarks.landmark[0].x,\n",
    "                               results.right_hand_landmarks.landmark[0].y,\n",
    "                               results.right_hand_landmarks.landmark[0].z])\n",
    "                dist_norm = np.linalg.norm(lw - rw)\n",
    "                scale_factor = 2.0\n",
    "                distance_meters = dist_norm * scale_factor\n",
    "                dist_text = f\"Hands Distance: {distance_meters:.2f} m\"\n",
    "                draw_text_with_shadow(image, dist_text, (15, base_y + len(hands_open_messages)*30), 0.7, (255,255,0), 2)\n",
    "\n",
    "            # FER emotion detection every few frames for efficiency\n",
    "            if frame_counter % emotion_frame_skip == 0:\n",
    "                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                cached_emotions = emotion_detector.detect_emotions(rgb_image)\n",
    "\n",
    "            # Use heuristic sleepy/active detection\n",
    "            custom_emotion = None\n",
    "            if results.face_landmarks:\n",
    "                custom_emotion = detect_sleepy_active(results.face_landmarks, w, h)\n",
    "\n",
    "            # FER dominant emotion or fallback to Neutral\n",
    "            if cached_emotions:\n",
    "                top_emotion = max(cached_emotions[0]['emotions'].items(), key=lambda x: x[1])\n",
    "                raw_emotion = f\"{top_emotion[0].capitalize()} ({top_emotion[1]*100:.1f}%)\"\n",
    "            else:\n",
    "                raw_emotion = \"Neutral\"\n",
    "\n",
    "            # Show custom if detected else smooth FER emotion label\n",
    "            display_emotion = custom_emotion if custom_emotion else smooth_emotion(raw_emotion)\n",
    "\n",
    "            # Display current emotion label on image\n",
    "            draw_text_with_shadow(image, display_emotion, (15, 80), 0.7, (0,0,255), 2)\n",
    "\n",
    "            # Pose confidence and posture evaluation\n",
    "            if results.pose_landmarks:\n",
    "                torso_ids = [11,12,23,24,25,26]\n",
    "                visibilities = [results.pose_landmarks.landmark[i].visibility for i in torso_ids]\n",
    "                avg_conf = sum(visibilities)/len(visibilities)\n",
    "                conf_str = f\"Pose Confidence: {avg_conf:.2f}\"\n",
    "\n",
    "                left_shoulder = results.pose_landmarks.landmark[11]\n",
    "                left_hip = results.pose_landmarks.landmark[23]\n",
    "                torso_vec = np.array([left_shoulder.x - left_hip.x, left_shoulder.y - left_hip.y])\n",
    "                vertical_vec = np.array([0,-1])\n",
    "                torso_vec_norm = torso_vec / (np.linalg.norm(torso_vec)+1e-6)\n",
    "                angle_rad = np.arccos(np.clip(np.dot(torso_vec_norm, vertical_vec), -1.0, 1.0))\n",
    "                angle_deg = np.degrees(angle_rad)\n",
    "                posture_str = \"Good Posture\" if angle_deg < 15 else \"Poor Posture\"\n",
    "\n",
    "                cv2.putText(image, conf_str, (15,40), cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                            (0,255,0) if avg_conf > 0.5 else (0,0,255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(image, posture_str, (15,110), cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                            (0,255,0) if posture_str == \"Good Posture\" else (0,0,255),2,cv2.LINE_AA)\n",
    "\n",
    "            # Display control instructions on bottom-left corner\n",
    "            info_text = \"q=quit | space=pause/resume | s=screenshot | c=theme\"\n",
    "            cv2.putText(image, info_text, (8, h-16), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.38, (80, 40, 10), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Show the final processed frame\n",
    "            cv2.imshow(window_name, image)\n",
    "\n",
    "        # Keyboard events for quit, pause/resume, screenshots, and theme switch\n",
    "        key = cv2.waitKey(10) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord(' '):\n",
    "            paused = not paused\n",
    "        elif key == ord('s') and not paused:\n",
    "            img_count += 1\n",
    "            cv2.imwrite(f\"screenshot_{img_count}.png\", image)\n",
    "            print(f\"Screenshot saved as screenshot_{img_count}.png\")\n",
    "        elif key == ord('c'):\n",
    "            theme_idx = (theme_idx + 1) % len(color_themes)\n",
    "\n",
    "# Release resources gracefully\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc05894-921c-4107-b48b-681db583a37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mediapipe_env)",
   "language": "python",
   "name": "mediapipe_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

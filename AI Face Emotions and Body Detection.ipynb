{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bade97c0-03c3-4230-97cc-dfa1252c66aa",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f0459a-a295-4859-85a2-b049bab7315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from fer import FER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd5b7a-bb97-431d-9523-6389e8ffe7c5",
   "metadata": {},
   "source": [
    "Helping function for drawing of lines and shadows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69c65e12-338e-4623-a6ae-34bce21674d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw text with a white shadow for better readability\n",
    "def draw_text_with_shadow(image, text, pos, font_scale, color, thickness):\n",
    "    x, y = pos\n",
    "    # Draw white shadow for contrast\n",
    "    cv2.putText(image, text, (x + 1, y + 1), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale, (255, 255, 255), thickness + 1, cv2.LINE_AA)\n",
    "    # Draw main text\n",
    "    cv2.putText(image, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale, color, thickness, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35536bc-9f6b-478b-bd0b-e4189023b5ac",
   "metadata": {},
   "source": [
    "Function for checking the hand opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d09a2f47-6ea8-42ed-982b-21d32dcdd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if hand is open based on finger positions and spread angle between index and pinky fingertips\n",
    "def is_hand_open(hand_landmarks, image_width, image_height):\n",
    "    wrist = hand_landmarks.landmark[0]\n",
    "    tips = [hand_landmarks.landmark[i] for i in [4, 8, 12, 16, 20]]  # Thumb and fingertips\n",
    "    wrist_y = wrist.y\n",
    "\n",
    "    # Count fingers raised (tips above wrist vertically), ignoring thumb\n",
    "    count_up = sum(lm.y < wrist_y for lm in tips[1:])\n",
    "\n",
    "    # Calculate spread angle between index and pinky fingertips\n",
    "    idx_tip = tips[1]\n",
    "    pinky_tip = tips[4]\n",
    "\n",
    "    idx_x, idx_y = int(idx_tip.x * image_width), int(idx_tip.y * image_height)\n",
    "    pinky_x, pinky_y = int(pinky_tip.x * image_width), int(pinky_tip.y * image_height)\n",
    "    wrist_x, wrist_y_px = int(wrist.x * image_width), int(wrist.y * image_height)\n",
    "\n",
    "    vec_idx = np.array([idx_x - wrist_x, idx_y - wrist_y_px])\n",
    "    vec_pinky = np.array([pinky_x - wrist_x, pinky_y - wrist_y_px])\n",
    "\n",
    "    # Normalize vectors to calculate angle safely\n",
    "    vec_idx_norm = vec_idx / (np.linalg.norm(vec_idx) + 1e-6)\n",
    "    vec_pinky_norm = vec_pinky / (np.linalg.norm(vec_pinky) + 1e-6)\n",
    "    angle_rad = np.arccos(np.clip(np.dot(vec_idx_norm, vec_pinky_norm), -1.0, 1.0))\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    # Hand is open if at least 4 fingers up and spread angle > 30 degrees\n",
    "    return count_up >= 4 and angle_deg > 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cea29-bb56-4909-a2da-5284fc05caaf",
   "metadata": {},
   "source": [
    "Initializing Mediapipe utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6eee4c8-9215-4829-a19d-8b66bcf2ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe holistic and drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c282b-5ff2-4426-bc6a-dfd16eb984e6",
   "metadata": {},
   "source": [
    "Detection of facial landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "243f19cc-c78e-4913-95dd-07c351d879c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important facial and body landmarks to label\n",
    "important_face_landmarks = {\n",
    "    \"Forehead\": 10, \"Eyes\": 33, \"Nose\": 1, \"Lips\": 61,\n",
    "    \"Chin\": 199, \"Left Ear\": 127, \"Right Ear\": 356\n",
    "}\n",
    "\n",
    "important_body_landmarks = {\n",
    "    \"Left Shoulder\": 11, \"Right Shoulder\": 12,\n",
    "    \"Left Elbow\": 13, \"Right Elbow\": 14,\n",
    "    \"Left Wrist\": 15, \"Right Wrist\": 16,\n",
    "    \"Left Hip\": 23, \"Right Hip\": 24,\n",
    "    \"Left Knee\": 25, \"Right Knee\": 26,\n",
    "    \"Left Ankle\": 27, \"Right Ankle\": 28\n",
    "}\n",
    "\n",
    "# Finger landmarks and their names\n",
    "finger_landmarks = {\n",
    "    0: \"Palm\", 4: \"Thumb Tip\", 8: \"Index Tip\",\n",
    "    12: \"Middle Tip\", 16: \"Ring Tip\", 20: \"Pinky Tip\"\n",
    "}\n",
    "\n",
    "# Text offsets for labeling landmarks for better positioning\n",
    "offsets = {\n",
    "    \"Left\": (-25, -12),\n",
    "    \"Right\": (23, -10),\n",
    "    \"default\": (10, -14)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b513b-f2a9-466b-a5e7-bee640c649ec",
   "metadata": {},
   "source": [
    "Styling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "165ad2ff-bf03-487e-8032-6b00a383cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing parameters\n",
    "line_thickness = 1\n",
    "circle_radius = 2\n",
    "font_scale = 0.35\n",
    "text_thickness = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba807e0-1d93-4830-b12b-97d0da8ee501",
   "metadata": {},
   "source": [
    "Emotion Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd822d46-939e-4861-810d-526c67728548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FER emotion detector with MTCNN for face detection\n",
    "emotion_detector = FER(mtcnn=True)\n",
    "\n",
    "# Keep track of fingertip trails (max length 20 points)\n",
    "max_trails_len = 20\n",
    "finger_trails_right = {i: deque(maxlen=max_trails_len) for i in finger_landmarks.keys()}\n",
    "finger_trails_left = {i: deque(maxlen=max_trails_len) for i in finger_landmarks.keys()}\n",
    "\n",
    "# Emotion smoothing to reduce jitter by majority in last 10 detections\n",
    "emotion_history = deque(maxlen=10)\n",
    "def smooth_emotion(emotion_label):\n",
    "    emotion_history.append(emotion_label)\n",
    "    most_common = max(set(emotion_history), key=emotion_history.count)\n",
    "    return most_common\n",
    "\n",
    "# Custom heuristic to detect sleepy or active state based on eye openness\n",
    "def detect_sleepy_active(face_landmarks, width, height):\n",
    "    try:\n",
    "        left_eye_upper = face_landmarks.landmark[159]\n",
    "        left_eye_lower = face_landmarks.landmark[145]\n",
    "        right_eye_upper = face_landmarks.landmark[386]\n",
    "        right_eye_lower = face_landmarks.landmark[374]\n",
    "\n",
    "        left_eye_open = abs(left_eye_upper.y - left_eye_lower.y)\n",
    "        right_eye_open = abs(right_eye_upper.y - right_eye_lower.y)\n",
    "        avg_eye_open = (left_eye_open + right_eye_open) / 2\n",
    "\n",
    "        sleepy_threshold = 0.008\n",
    "        active_threshold = 0.02\n",
    "\n",
    "        if avg_eye_open < sleepy_threshold:\n",
    "            return \"Sleepy\"\n",
    "        elif avg_eye_open > active_threshold:\n",
    "            return \"Active\"\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d74d7b-2c90-4704-a02e-5559f2bf5902",
   "metadata": {},
   "source": [
    "Setting up the webcam (upto full screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cf2ef3b-34f3-498b-bad5-6dc7d0dd5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up webcam capture and full screen window\n",
    "cap = cv2.VideoCapture(0)\n",
    "window_name = 'MediaPipe + FER Emotion Detection'\n",
    "cv2.namedWindow(window_name, cv2.WND_PROP_FULLSCREEN)\n",
    "cv2.setWindowProperty(window_name, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "paused = False\n",
    "img_count = 0\n",
    "theme_idx = 0\n",
    "\n",
    "# Define some color themes for drawing\n",
    "color_themes = [\n",
    "    {'dot': (0, 0, 255), 'line': (0, 0, 0)},         # Red dots, black lines\n",
    "    {'dot': (0, 255, 0), 'line': (160, 32, 240)},   # Green dots, purple lines\n",
    "    {'dot': (255, 127, 36), 'line': (30, 30, 30)}   # Orange dots, dark gray lines\n",
    "]\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        if not paused:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert image color space for MediaPipe processing\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image_rgb)\n",
    "            image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "            h, w, _ = image.shape\n",
    "            color_theme = color_themes[theme_idx]\n",
    "\n",
    "            # Draw and label important face landmarks\n",
    "            if results.face_landmarks:\n",
    "                for name, idx in important_face_landmarks.items():\n",
    "                    try:\n",
    "                        lm = results.face_landmarks.landmark[idx]\n",
    "                        x, y = int(lm.x * w), int(lm.y * h)\n",
    "                        cv2.circle(image, (x, y), circle_radius, color_theme['dot'], -1, cv2.LINE_AA)\n",
    "                        dx, dy = offsets[\"default\"]\n",
    "                        draw_text_with_shadow(image, name, (x + dx, y + dy), font_scale, (0, 0, 0), text_thickness)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            # Draw and label important body landmarks\n",
    "            if results.pose_landmarks:\n",
    "                for name, idx in important_body_landmarks.items():\n",
    "                    try:\n",
    "                        lm = results.pose_landmarks.landmark[idx]\n",
    "                        x, y = int(lm.x * w), int(lm.y * h)\n",
    "                        cv2.circle(image, (x, y), circle_radius + 1, color_theme['dot'], -1, cv2.LINE_AA)\n",
    "                        if \"Left\" in name:\n",
    "                            dx, dy = offsets[\"Left\"]\n",
    "                        elif \"Right\" in name:\n",
    "                            dx, dy = offsets[\"Right\"]\n",
    "                        else:\n",
    "                            dx, dy = offsets[\"default\"]\n",
    "                        draw_text_with_shadow(image, name, (x + dx, y + dy), font_scale, (0, 0, 0), text_thickness)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                # Draw pose connections using MediaPipe utility\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_holistic.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness, circle_radius=circle_radius),\n",
    "                    mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness)\n",
    "                )\n",
    "\n",
    "            # Process hand landmarks, fingertip trails, labels, and open-hand gesture lines\n",
    "            # Also display messages if hands are open\n",
    "            hands_open_messages = []\n",
    "            for hand_side, hand_landmarks, finger_trails in [\n",
    "                (\"Right\", results.right_hand_landmarks, finger_trails_right),\n",
    "                (\"Left\", results.left_hand_landmarks, finger_trails_left)\n",
    "            ]:\n",
    "                if hand_landmarks:\n",
    "                    for idx, part in finger_landmarks.items():\n",
    "                        try:\n",
    "                            lm = hand_landmarks.landmark[idx]\n",
    "                            x, y = int(lm.x * w), int(lm.y * h)\n",
    "                            finger_trails[idx].append((x, y))\n",
    "                            cv2.circle(image, (x, y), circle_radius + 1, color_theme['dot'], -1, cv2.LINE_AA)\n",
    "                            dx, dy = offsets[hand_side]\n",
    "                            draw_text_with_shadow(image, f\"{hand_side} {part}\", (x + dx, y + dy), font_scale, (0, 0, 0), text_thickness)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                    # Draw hand connections\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp_holistic.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness, circle_radius=circle_radius),\n",
    "                        mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness)\n",
    "                    )\n",
    "\n",
    "                    # If hand is open, draw lines from wrist to fingertips\n",
    "                    if is_hand_open(hand_landmarks, w, h):\n",
    "                        wrist_lm = hand_landmarks.landmark[0]\n",
    "                        wrist_xy = (int(wrist_lm.x * w), int(wrist_lm.y * h))\n",
    "                        fingertip_ids = [4, 8, 12, 16, 20]\n",
    "                        for tip_idx in fingertip_ids:\n",
    "                            tip_lm = hand_landmarks.landmark[tip_idx]\n",
    "                            tip_xy = (int(tip_lm.x * w), int(tip_lm.y * h))\n",
    "                            cv2.line(image, wrist_xy, tip_xy, (0, 255, 255), 2)\n",
    "                        # Add message for open hand\n",
    "                        hands_open_messages.append(f\"{hand_side} Hand Open\")\n",
    "\n",
    "                    # Draw fingertip trails as lines\n",
    "                    for trail in finger_trails.values():\n",
    "                        for i in range(1, len(trail)):\n",
    "                            cv2.line(image, trail[i - 1], trail[i], (0, 255, 255), 1)\n",
    "\n",
    "            # Display open hand messages on screen, vertically spaced\n",
    "            base_y = 140\n",
    "            for i, msg in enumerate(hands_open_messages):\n",
    "                draw_text_with_shadow(image, msg, (15, base_y + i * 30), 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # If both hands detected, calculate and display distance in meters (approximate)\n",
    "            if results.left_hand_landmarks and results.right_hand_landmarks:\n",
    "                left_wrist = results.left_hand_landmarks.landmark[0]\n",
    "                right_wrist = results.right_hand_landmarks.landmark[0]\n",
    "\n",
    "                # Normalized 3D coordinates\n",
    "                lw = np.array([left_wrist.x, left_wrist.y, left_wrist.z])\n",
    "                rw = np.array([right_wrist.x, right_wrist.y, right_wrist.z])\n",
    "\n",
    "                # Euclidean distance in normalized coordinate space\n",
    "                dist_norm = np.linalg.norm(lw - rw)\n",
    "\n",
    "                # Approximate scale:\n",
    "                # At ~1 meter from camera, normalized distance ~0.3 corresponds roughly to ~0.6 m\n",
    "                # You can calibrate scale_factor as needed\n",
    "                scale_factor = 2.0  # empirical scale factor for meters conversion\n",
    "                distance_meters = dist_norm * scale_factor\n",
    "\n",
    "                distance_text = f\"Hands Distance: {distance_meters:.2f} m\"\n",
    "                draw_text_with_shadow(image, distance_text, (15, base_y + len(hands_open_messages) * 30), 0.7, (255, 255, 0), 2)\n",
    "\n",
    "            # Emotion detection using FER on the current frame\n",
    "            rgb_frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            emotions = emotion_detector.detect_emotions(rgb_frame)\n",
    "\n",
    "            # Detect sleepy or active states based on eye openness (heuristic)\n",
    "            custom_emotion = None\n",
    "            if results.face_landmarks:\n",
    "                custom_emotion = detect_sleepy_active(results.face_landmarks, w, h)\n",
    "\n",
    "            # Get the highest detected emotion if any\n",
    "            if emotions:\n",
    "                top_emotion = max(emotions[0]['emotions'].items(), key=lambda x: x[1])\n",
    "                raw_emotion = f\"{top_emotion[0].capitalize()} ({top_emotion[1] * 100:.1f}%)\"\n",
    "            else:\n",
    "                raw_emotion = \"Neutral\"\n",
    "\n",
    "            # Use custom detected emotion if available, else apply smoothing on raw emotion\n",
    "            display_emotion = custom_emotion if custom_emotion else smooth_emotion(raw_emotion)\n",
    "\n",
    "            # Display the emotion label on the image\n",
    "            draw_text_with_shadow(image, display_emotion, (15, 80), 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            # Pose confidence (average visibility) and evaluate sitting posture\n",
    "            if results.pose_landmarks:\n",
    "                torso_ids = [11, 12, 23, 24, 25, 26]  # Shoulders, hips, knees\n",
    "                visibilities = [results.pose_landmarks.landmark[i].visibility for i in torso_ids]\n",
    "                avg_conf = sum(visibilities) / len(visibilities)\n",
    "                confidence_str = f\"Pose Confidence: {avg_conf:.2f}\"\n",
    "\n",
    "                # Calculate torso vertical angle using left shoulder and left hip\n",
    "                left_shoulder = results.pose_landmarks.landmark[11]\n",
    "                left_hip = results.pose_landmarks.landmark[23]\n",
    "                torso_vec = np.array([left_shoulder.x - left_hip.x, left_shoulder.y - left_hip.y])\n",
    "                vertical_vec = np.array([0, -1])\n",
    "                torso_vec_norm = torso_vec / (np.linalg.norm(torso_vec) + 1e-6)\n",
    "                angle_rad = np.arccos(np.clip(np.dot(torso_vec_norm, vertical_vec), -1.0, 1.0))\n",
    "                angle_deg = np.degrees(angle_rad)\n",
    "                posture_str = \"Good Posture\" if angle_deg < 15 else \"Poor Posture\"\n",
    "\n",
    "                # Display pose confidence and posture assessment on frame\n",
    "                cv2.putText(image, confidence_str, (15, 40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.6, (0, 255, 0) if avg_conf > 0.5 else (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(image, posture_str, (15, 110), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.6, (0, 255, 0) if posture_str == \"Good Posture\" else (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Instructions on controls displayed at the screen bottom-left\n",
    "            info_text = \"q=quit | space=pause/resume | s=screenshot | c=theme\"\n",
    "            cv2.putText(image, info_text, (8, h - 16), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.38, (80, 40, 10), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Show the processed frame in fullscreen window\n",
    "            cv2.imshow(window_name, image)\n",
    "\n",
    "        # Keyboard event handling\n",
    "        key = cv2.waitKey(10) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord(' '):\n",
    "            paused = not paused\n",
    "        elif key == ord('s') and not paused:\n",
    "            img_count += 1\n",
    "            cv2.imwrite(f\"screenshot_{img_count}.png\", image)\n",
    "            print(f\"Screenshot saved as screenshot_{img_count}.png\")\n",
    "        elif key == ord('c'):\n",
    "            theme_idx = (theme_idx + 1) % len(color_themes)\n",
    "\n",
    "# Release resources after loop exits\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9891d2-cf86-4b29-9c6b-29c7435400db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

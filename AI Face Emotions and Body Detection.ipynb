{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bade97c0-03c3-4230-97cc-dfa1252c66aa",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f0459a-a295-4859-85a2-b049bab7315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress specific warning related to Keras input structure from FER library\n",
    "warnings.filterwarnings(\"ignore\", message=\"The structure of `inputs` doesn't match the expected structure.\")\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from fer import FER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd5b7a-bb97-431d-9523-6389e8ffe7c5",
   "metadata": {},
   "source": [
    "Helping function for drawing of lines and shadows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69c65e12-338e-4623-a6ae-34bce21674d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw text with a white shadow behind for readability on any background\n",
    "def draw_text_with_shadow(image, text, pos, font_scale, color, thickness):\n",
    "    x, y = pos\n",
    "    cv2.putText(image, text, (x + 1, y + 1), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale, (255, 255, 255), thickness + 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                font_scale, color, thickness, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35536bc-9f6b-478b-bd0b-e4189023b5ac",
   "metadata": {},
   "source": [
    "Function for checking the hand opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d09a2f47-6ea8-42ed-982b-21d32dcdd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if hand is open by counting raised fingers and checking finger spread angle\n",
    "def is_hand_open(hand_landmarks, image_width, image_height):\n",
    "    wrist = hand_landmarks.landmark[0]\n",
    "    tips = [hand_landmarks.landmark[i] for i in [4, 8, 12, 16, 20]]\n",
    "    wrist_y = wrist.y\n",
    "\n",
    "    # Count fingers raised above wrist vertically (exclude thumb at tips[0])\n",
    "    count_up = sum(lm.y < wrist_y for lm in tips[1:])\n",
    "    \n",
    "    # Calculate spread angle between index and pinky fingertips\n",
    "    idx_tip = tips[1]\n",
    "    pinky_tip = tips[4]\n",
    "    idx_x, idx_y = int(idx_tip.x * image_width), int(idx_tip.y * image_height)\n",
    "    pinky_x, pinky_y = int(pinky_tip.x * image_width), int(pinky_tip.y * image_height)\n",
    "    wrist_x, wrist_y_px = int(wrist.x * image_width), int(wrist.y * image_height)\n",
    "\n",
    "    vec_idx = np.array([idx_x - wrist_x, idx_y - wrist_y_px])\n",
    "    vec_pinky = np.array([pinky_x - wrist_x, pinky_y - wrist_y_px])\n",
    "\n",
    "    vec_idx_norm = vec_idx / (np.linalg.norm(vec_idx) + 1e-6)\n",
    "    vec_pinky_norm = vec_pinky / (np.linalg.norm(vec_pinky) + 1e-6)\n",
    "    angle_rad = np.arccos(np.clip(np.dot(vec_idx_norm, vec_pinky_norm), -1.0, 1.0))\n",
    "    angle_deg = np.degrees(angle_rad)\n",
    "\n",
    "    # Hand is open if at least 4 fingers raised and spread angle > 30 degrees\n",
    "    return count_up >= 4 and angle_deg > 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cea29-bb56-4909-a2da-5284fc05caaf",
   "metadata": {},
   "source": [
    "Initializing Mediapipe utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6eee4c8-9215-4829-a19d-8b66bcf2ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe holistic and drawing utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c282b-5ff2-4426-bc6a-dfd16eb984e6",
   "metadata": {},
   "source": [
    "Detection of facial landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "243f19cc-c78e-4913-95dd-07c351d879c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facial landmarks to label\n",
    "important_face_landmarks = {\n",
    "    \"Forehead\": 10, \"Eyes\": 33, \"Nose\": 1, \"Lips\": 61,\n",
    "    \"Chin\": 199, \"Left Ear\": 127, \"Right Ear\": 356\n",
    "}\n",
    "\n",
    "# Body landmarks to label and assess\n",
    "important_body_landmarks = {\n",
    "    \"Left Shoulder\": 11, \"Right Shoulder\": 12,\n",
    "    \"Left Elbow\": 13, \"Right Elbow\": 14,\n",
    "    \"Left Wrist\": 15, \"Right Wrist\": 16,\n",
    "    \"Left Hip\": 23, \"Right Hip\": 24,\n",
    "    \"Left Knee\": 25, \"Right Knee\": 26,\n",
    "    \"Left Ankle\": 27, \"Right Ankle\": 28\n",
    "}\n",
    "\n",
    "# Finger landmarks mapping to names\n",
    "finger_landmarks = {\n",
    "    0: \"Palm\", 4: \"Thumb Tip\", 8: \"Index Tip\",\n",
    "    12: \"Middle Tip\", 16: \"Ring Tip\", 20: \"Pinky Tip\"\n",
    "}\n",
    "\n",
    "# Offsets for text labeling\n",
    "offsets = {\n",
    "    \"Left\": (-25, -12),\n",
    "    \"Right\": (23, -10),\n",
    "    \"default\": (10, -14)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b513b-f2a9-466b-a5e7-bee640c649ec",
   "metadata": {},
   "source": [
    "Styling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "165ad2ff-bf03-487e-8032-6b00a383cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing parameters\n",
    "line_thickness = 1\n",
    "circle_radius = 2\n",
    "font_scale = 0.35\n",
    "text_thickness = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba807e0-1d93-4830-b12b-97d0da8ee501",
   "metadata": {},
   "source": [
    "Emotion Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd822d46-939e-4861-810d-526c67728548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FER facial emotion detector with MTCNN for better accuracy\n",
    "emotion_detector = FER(mtcnn=True)\n",
    "\n",
    "# For fingertip trails visualization\n",
    "max_trails_len = 20\n",
    "finger_trails_right = {i: deque(maxlen=max_trails_len) for i in finger_landmarks.keys()}\n",
    "finger_trails_left = {i: deque(maxlen=max_trails_len) for i in finger_landmarks.keys()}\n",
    "\n",
    "# For smoothing emotion output to reduce jitter\n",
    "emotion_history = deque(maxlen=10)\n",
    "def smooth_emotion(emotion_label):\n",
    "    emotion_history.append(emotion_label)\n",
    "    return max(set(emotion_history), key=emotion_history.count)\n",
    "\n",
    "# Custom heuristic for sleepy/active state based on eye openness landmarks\n",
    "def detect_sleepy_active(face_landmarks, width, height):\n",
    "    try:\n",
    "        left_eye_upper = face_landmarks.landmark[159]\n",
    "        left_eye_lower = face_landmarks.landmark[145]\n",
    "        right_eye_upper = face_landmarks.landmark[386]\n",
    "        right_eye_lower = face_landmarks.landmark[374]\n",
    "\n",
    "        left_eye_oen = abs(left_eye_upper.y - left_eye_lower.y)\n",
    "        right_eye_open = abs(right_eye_upper.y - right_eye_lower.y)\n",
    "        avg_eye_open = (left_eye_open + right_eye_open) / 2\n",
    "\n",
    "        sleepy_threshold = 0.008\n",
    "        active_threshold = 0.02\n",
    "\n",
    "        if avg_eye_open < sleepy_threshold:\n",
    "            return \"Sleepy\"\n",
    "        elif avg_eye_open > active_threshold:\n",
    "            return \"Active\"\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d74d7b-2c90-4704-a02e-5559f2bf5902",
   "metadata": {},
   "source": [
    "Setting up the webcam (upto full screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cf2ef3b-34f3-498b-bad5-6dc7d0dd5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup webcam feed and fullscreen window\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)   # Reduce resolution for speed (adjust as needed)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "window_name = 'MediaPipe + FER Emotion Detection'\n",
    "cv2.namedWindow(window_name, cv2.WND_PROP_FULLSCREEN)\n",
    "cv2.setWindowProperty(window_name, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "\n",
    "paused = False\n",
    "img_count = 0\n",
    "theme_idx = 0\n",
    "\n",
    "# Color themes for landmark drawing\n",
    "color_themes = [\n",
    "    {'dot': (0, 0, 255), 'line': (0, 0, 0)},         # Red dots, black lines\n",
    "    {'dot': (0, 255, 0), 'line': (160, 32, 240)},   # Green dots, purple lines\n",
    "    {'dot': (255, 127, 36), 'line': (30, 30, 30)}   # Orange dots, dark gray lines\n",
    "]\n",
    "\n",
    "# Frame counter for skipping emotion analysis frames to improve speed\n",
    "emotion_frame_skip = 5\n",
    "frame_counter = 0\n",
    "cached_emotions = []\n",
    "cached_emotion_label = \"Neutral\"\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        if not paused:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert frame to RGB for MediaPipe\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = holistic.process(image_rgb)\n",
    "\n",
    "            # Convert back to BGR for OpenCV display\n",
    "            image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "            h, w, _ = image.shape\n",
    "            color_theme = color_themes[theme_idx]\n",
    "\n",
    "            # Draw and label face landmarks\n",
    "            if results.face_landmarks:\n",
    "                for name, idx in important_face_landmarks.items():\n",
    "                    try:\n",
    "                        lm = results.face_landmarks.landmark[idx]\n",
    "                        x, y = int(lm.x * w), int(lm.y * h)\n",
    "                        cv2.circle(image, (x, y), circle_radius, color_theme['dot'], -1, cv2.LINE_AA)\n",
    "                        dx, dy = offsets[\"default\"]\n",
    "                        draw_text_with_shadow(image, name, (x + dx, y + dy), font_scale, (0, 0, 0), text_thickness)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            # Draw and label body landmarks\n",
    "            if results.pose_landmarks:\n",
    "                for name, idx in important_body_landmarks.items():\n",
    "                    try:\n",
    "                        lm = results.pose_landmarks.landmark[idx]\n",
    "                        x, y = int(lm.x * w), int(lm.y * h)\n",
    "                        cv2.circle(image, (x, y), circle_radius + 1, color_theme['dot'], -1, cv2.LINE_AA)\n",
    "                        if \"Left\" in name:\n",
    "                            dx, dy = offsets[\"Left\"]\n",
    "                        elif \"Right\" in name:\n",
    "                            dx, dy = offsets[\"Right\"]\n",
    "                        else:\n",
    "                            dx, dy = offsets[\"default\"]\n",
    "                        draw_text_with_shadow(image, name, (x + dx, y + dy), font_scale, (0, 0, 0), text_thickness)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_holistic.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness, circle_radius=circle_radius),\n",
    "                    mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness)\n",
    "                )\n",
    "\n",
    "            # Process hands with fingertip trails and open hand detection messages\n",
    "            hands_open_messages = []\n",
    "            for hand_side, hand_landmarks, finger_trails in [\n",
    "                (\"Right\", results.right_hand_landmarks, finger_trails_right),\n",
    "                (\"Left\", results.left_hand_landmarks, finger_trails_left)\n",
    "            ]:\n",
    "                if hand_landmarks:\n",
    "                    # Draw fingertips and label them\n",
    "                    for idx, part in finger_landmarks.items():\n",
    "                        try:\n",
    "                            lm = hand_landmarks.landmark[idx]\n",
    "                            x, y = int(lm.x * w), int(lm.y * h)\n",
    "                            finger_trails[idx].append((x, y))\n",
    "                            cv2.circle(image, (x, y), circle_radius + 1, color_theme['dot'], -1, cv2.LINE_AA)\n",
    "                            dx, dy = offsets[hand_side]\n",
    "                            draw_text_with_shadow(image, f\"{hand_side} {part}\", (x + dx, y + dy), font_scale, (0, 0, 0), text_thickness)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                    # Draw hand skeleton connections\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image,\n",
    "                        hand_landmarks,\n",
    "                        mp_holistic.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness, circle_radius=circle_radius),\n",
    "                        mp_drawing.DrawingSpec(color=color_theme['line'], thickness=line_thickness)\n",
    "                    )\n",
    "\n",
    "                    # If hand is open, add a message (yellow wrist-to-fingertip lines removed)\n",
    "                    if is_hand_open(hand_landmarks, w, h):\n",
    "                        hands_open_messages.append(f\"{hand_side} Hand Open\")\n",
    "\n",
    "                    # Draw fingertip trails as lines\n",
    "                    for trail in finger_trails.values():\n",
    "                        for i in range(1, len(trail)):\n",
    "                            cv2.line(image, trail[i - 1], trail[i], (0, 255, 255), 1)\n",
    "\n",
    "            # Display hand open messages with spacing\n",
    "            base_y = 140\n",
    "            for i, msg in enumerate(hands_open_messages):\n",
    "                draw_text_with_shadow(image, msg, (15, base_y + i * 30), 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # If both hands detected, calculate and display approximate wrist distance in meters\n",
    "            if results.left_hand_landmarks and results.right_hand_landmarks:\n",
    "                left_wrist = results.left_hand_landmarks.landmark[0]\n",
    "                right_wrist = results.right_hand_landmarks.landmark[0]\n",
    "\n",
    "                lw = np.array([left_wrist.x, left_wrist.y, left_wrist.z])\n",
    "                rw = np.array([right_wrist.x, right_wrist.y, right_wrist.z])\n",
    "\n",
    "                dist_norm = np.linalg.norm(lw - rw)\n",
    "                scale_factor = 2.0  # Scale factor for approximate meters conversion\n",
    "                distance_meters = dist_norm * scale_factor\n",
    "\n",
    "                dist_text = f\"Hands Distance: {distance_meters:.2f} m\"\n",
    "                draw_text_with_shadow(image, dist_text, (15, base_y + len(hands_open_messages) * 30), 0.7, (255, 255, 0), 2)\n",
    "\n",
    "            # Update frame counter, run emotion inference every N frames for efficiency\n",
    "            frame_counter += 1\n",
    "            if frame_counter % emotion_frame_skip == 0:\n",
    "                rgb_frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                cached_emotions = emotion_detector.detect_emotions(rgb_frame)\n",
    "\n",
    "            # Detect sleepy or active custom emotion\n",
    "            custom_emotion = None\n",
    "            if results.face_landmarks:\n",
    "                custom_emotion = detect_sleepy_active(results.face_landmarks, w, h)\n",
    "\n",
    "            # Extract top FER emotion or use neutral\n",
    "            if cached_emotions:\n",
    "                top_emotion = max(cached_emotions[0]['emotions'].items(), key=lambda x: x[1])\n",
    "                raw_emotion = f\"{top_emotion[0].capitalize()} ({top_emotion[1] * 100:.1f}%)\"\n",
    "            else:\n",
    "                raw_emotion = \"Neutral\"\n",
    "\n",
    "            # Smooth emotion output or show custom state\n",
    "            cached_emotion_label = custom_emotion if custom_emotion else smooth_emotion(raw_emotion)\n",
    "\n",
    "            # Display emotion on screen\n",
    "            draw_text_with_shadow(image, cached_emotion_label, (15, 80), 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            # Pose confidence and posture metrics\n",
    "            if results.pose_landmarks:\n",
    "                torso_ids = [11, 12, 23, 24, 25, 26]\n",
    "                visibilities = [results.pose_landmarks.landmark[i].visibility for i in torso_ids]\n",
    "                avg_conf = sum(visibilities) / len(visibilities)\n",
    "                confidence_str = f\"Pose Confidence: {avg_conf:.2f}\"\n",
    "\n",
    "                left_shoulder = results.pose_landmarks.landmark[11]\n",
    "                left_hip = results.pose_landmarks.landmark[23]\n",
    "                torso_vec = np.array([left_shoulder.x - left_hip.x, left_shoulder.y - left_hip.y])\n",
    "                vertical_vec = np.array([0, -1])\n",
    "                torso_vec_norm = torso_vec / (np.linalg.norm(torso_vec) + 1e-6)\n",
    "                angle_rad = np.arccos(np.clip(np.dot(torso_vec_norm, vertical_vec), -1.0, 1.0))\n",
    "                angle_deg = np.degrees(angle_rad)\n",
    "                posture_str = \"Good Posture\" if angle_deg < 15 else \"Poor Posture\"\n",
    "\n",
    "                cv2.putText(image, confidence_str, (15, 40), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.6, (0, 255, 0) if avg_conf > 0.5 else (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.putText(image, posture_str, (15, 110), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.6, (0, 255, 0) if posture_str == \"Good Posture\" else (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Display control instructions\n",
    "            info_text = \"q=quit | space=pause/resume | s=screenshot | c=theme\"\n",
    "            cv2.putText(image, info_text, (8, h - 16), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.38, (80, 40, 10), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Show final frame\n",
    "            cv2.imshow(window_name, image)\n",
    "\n",
    "        # Handle keyboard input regardless of pause state\n",
    "        key = cv2.waitKey(10) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord(' '):\n",
    "            paused = not paused\n",
    "        elif key == ord('s') and not paused:\n",
    "            img_count += 1\n",
    "            cv2.imwrite(f\"screenshot_{img_count}.png\", image)\n",
    "            print(f\"Screenshot saved as screenshot_{img_count}.png\")\n",
    "        elif key == ord('c'):\n",
    "            theme_idx = (theme_idx + 1) % len(color_themes)\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9891d2-cf86-4b29-9c6b-29c7435400db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
